{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Silence of the Integers\n",
    "\n",
    "The following notebook highlights the problem of silent integer overrun in numpy and scipy. Both, numpy and scipy don't check integer overflow in arrays or matrices which has the consequence that one might end up with inconsistent data despite correct code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy import special\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has been tested with the following versions of numpy and scipy:\n",
    "    - scipy version=1.2.0\n",
    "    - numpy version=1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'scipy version={scipy.__version__}')\n",
    "print(f'numpy version={np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The scipy problem\n",
    "\n",
    "Suppose we want to create a co-occurrence matrix from some observations. We've collected the base set of items (i.e. our vocabulary), so we can just record the row and column indices of the observed item - alongside its actual value.\n",
    "\n",
    "As the code below shows, we're trying to be efficient about the size of the `dtype` we're using. Maybe through domain knowledge or other, we know that we're never going to encounter a value larger than 255, or maybe we cap any value at 100.\n",
    "\n",
    "This is somewhat artificial, but highlights the actual problem. Theoretically, even a `np.uint64` might be too small for the number of observations you have.\n",
    "\n",
    "The problem comes at element 2/2 - we have 2 observations that sum up to 256, but our dtype's max value is 255. Lets see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create some co-occurrence data\n",
    "rows = np.array([1, 1, 1, 1, 2, 2, 3, 1, 1, 2], dtype=np.uint8)\n",
    "cols = np.array([0, 1, 2, 3, 0, 2, 1, 0, 1, 2], dtype=np.uint8)\n",
    "data = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 255], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a sparse matrix for these observations\n",
    "S = sparse.csr_matrix((data, (rows, cols)))\n",
    "S.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, _nothing_ happened. And that is precisely the problem. The element at 2/2 has silently overrun to 0.\n",
    "\n",
    "But what if we explicitely specify a larger `dtype` when we create the sparse matrix? Surely this is going to prevent the overflow! After all, all values in our data array are `<= 255`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sparse.csr_matrix((data, (rows, cols)), dtype=np.uint16)\n",
    "S.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh...same problem again...not cool!\n",
    "\n",
    "What we need to do is explicitly upcasting the data array, so any sort of gamble on using the smallest possible `dtype` for our data is more or less for nought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sparse.csr_matrix((data.astype(np.uint16), (rows, cols)))\n",
    "S.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More of the same\n",
    "\n",
    "Suppose we're gathering our co-occurrence data periodically from some incoming stream and accumulate the data in \"master\" matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create some more co-occurrence data, from some sort of incoming stream\n",
    "rows = np.array([1, 1, 1, 1, 2, 2, 3, 1, 1, 2], dtype=np.uint8)\n",
    "cols = np.array([0, 1, 2, 3, 0, 2, 1, 0, 1, 2], dtype=np.uint8)\n",
    "data = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 13], dtype=np.uint8)\n",
    "\n",
    "# S is the kind of master matrix, accumulating everything\n",
    "S = sparse.csr_matrix((data, (rows, cols)))\n",
    "S.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all looks well, lets wait for the next bunch of data from the stream..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have some more co-occurrence data\n",
    "rows = np.array([1, 2, 2, 2], dtype=np.uint8)\n",
    "cols = np.array([0, 0, 2, 2], dtype=np.uint8)\n",
    "data = np.array([1, 1, 1, 250], dtype=np.uint8)\n",
    "\n",
    "T = sparse.csr_matrix((data, (rows, cols)), shape=S.shape)\n",
    "T.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data look good, so just add them to our master matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets accumulate the co-occurrence data\n",
    "U = S + T\n",
    "U.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn...position 2/2 again overflowed silently.\n",
    "\n",
    "OK, you might argue, adding sparse matrices is really a bit of an edge case maybe. But what if we wanted to scale our data a bit? Lets recall the contents of `S` first, and then do the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = S * 20\n",
    "V.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn, `14 * 20` is certainly _NOT_ `24`...\n",
    "\n",
    "##### The numpy problem\n",
    "\n",
    "But you know what, our data is not that sparse and not that big really. We can just use numpy instead of scipy. Surely, numpy is a bit more forgiving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ud = S.toarray() + T.toarray()\n",
    "Ud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the actual F...???!!! But, but, lets try again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vd = S.toarray() * 20\n",
    "Vd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can not be real??!! numpy just overflows as silently as scipy!! \n",
    "\n",
    "This must be the point where we declare that god is dead and become nihilists...\n",
    "\n",
    "For numpy too, we need to explicitly upcast the dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ud = S.toarray().astype(np.uint16) + T.toarray().astype(np.uint16)\n",
    "Ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vd = S.toarray().astype(np.uint16) * 20\n",
    "Vd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, maybe we're just not very smart. So lets squiggle through the docs and see what we find. Ah yes, there it is, [numpy.seterr](https://docs.scipy.org/doc/numpy/reference/generated/numpy.seterr.html) looks precisely like the thing we're looking for! _Slightly_ inconvenient that the default behaviour is a silent overflow, but no worries, there is hope! As expected, the scalar silently overflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.uint8(255) * np.uint8(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we just set the error to `raise`, this way we'll hit a hard error if any integer should overflow! Much better than all this silence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = np.seterr(over='raise')\n",
    "np.uint8(255) * np.uint8(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmmmhhh, the beautiful smell of a fresh overflow error :D, looks like we're making progress! Lets go to our arrays again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ud = S.toarray() + T.toarray()\n",
    "Ud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait, WHAT???!!!!** You mean, this doesn't work for arrays???!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vd = S.toarray() * 20\n",
    "Vd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, it doesn't! Lets try with scipy again and hope against hope that maybe scipy behaves differently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy also offers some error behaviour customisation...\n",
    "_ = special.seterr(overflow='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = S * 20\n",
    "V.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, no, no, silent integer overflows all over the place. Again, while the examples in this notebook are slightly artificial due to using `np.uint8`'s, they illustrate the problem. If you're using 32bit types, then overflows can happen quicker than you might think, `np.int32`'s max value is \"only\" `2147483647` and `np.uint32`'s max value is twice that (`4294967295`), so depending on how many observations you might have, this can be enough - or not. And when its not, it will become _really_ hard to find the error in your data. Oh, and obviously even the whooping max value of `np.uint64` (`18446744073709551615`) might silently overflow at some point..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
